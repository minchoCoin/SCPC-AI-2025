{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df8f48f",
   "metadata": {},
   "source": [
    "```\n",
    "pandas==2.2.2\n",
    "torch==2.6.0+cu124\n",
    "torchaudio==2.6.0+cu124\n",
    "turchvision==0.21.0+cu124\n",
    "pillow==11.2.1\n",
    "transformers==4.53.2\n",
    "numpy==2.0.2\n",
    "tqdm==4.67.1\n",
    "scikit-learn==1.6.1\n",
    "peft==0.16.0\n",
    "```\n",
    "python==3.11.13\n",
    "\n",
    "OS==Ubuntu 22.04.4 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq scpc_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1fa4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.amp import autocast, GradScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2abfa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 41\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31670c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\"\n",
    "        self.processor_name = \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\"\n",
    "        self.train_csv_path = 'train_combined.csv'\n",
    "        self.test_csv_path = 'test.csv'\n",
    "        self.sample_submission_path = 'sample_submission.csv'\n",
    "        self.output_submission_path = 'submission_pluto.csv'\n",
    "        self.image_base_path = './'\n",
    "        self.batch_size = 64  # 배치 크기 증가\n",
    "        self.num_epochs = 6\n",
    "        self.learning_rate = 1e-4  # 학습률 감소\n",
    "        self.weight_decay = 1.5e-2\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.num_choices = 4\n",
    "        self.validation_split = 0.15\n",
    "        self.patience = 3  # Early stopping\n",
    "        self.save_best_model = True\n",
    "        self.model_save_path = 'best_clip_vqa_model.pth'\n",
    "        # LoRA 설정\n",
    "        self.lora_r = 32  # rank\n",
    "        self.lora_alpha = 64  # alpha\n",
    "        self.lora_dropout = 0.1\n",
    "\n",
    "        self.lora_target_modules = [\n",
    "            \"q_proj\",\n",
    "            \"v_proj\",\n",
    "            #\"k_proj\",\n",
    "            \"out_proj\",  # attention layers\n",
    "           # \"fc1\", \"fc2\",  # feed forward layers\n",
    "           # \"visual_projection\", #\"text_projection\"  # projection layers\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "print(f\"Using device: {config.device}\")\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Batch size: {config.batch_size}\")\n",
    "print(f\"Learning rate: {config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, df, processor, image_base_path, is_train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.image_base_path = image_base_path\n",
    "        self.is_train = is_train\n",
    "        self.num_choices = config.num_choices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_base_path, row[\"img_path\"])\n",
    "        question = row[\"Question\"]\n",
    "        choices = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            #logger.error(f\"Error loading image {image_path}: {e}\")\n",
    "            # 기본 이미지 생성 (검은색 이미지)\n",
    "            print('error: Unable to open image')\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "\n",
    "        # 질문과 각 선택지를 결합한 텍스트 생성\n",
    "        texts = [f\"Question: {question} Answer: {choice}\" for choice in choices]\n",
    "\n",
    "        # 이미지와 텍스트 인코딩\n",
    "        try:\n",
    "            # 이미지 인코딩\n",
    "            image_inputs = self.processor(\n",
    "                images=image,\n",
    "                return_tensors=\"pt\",\n",
    "                do_rescale=True,\n",
    "                do_normalize=True\n",
    "            )\n",
    "\n",
    "            # 텍스트 인코딩\n",
    "            text_inputs = self.processor(\n",
    "                text=texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                'pixel_values': image_inputs.pixel_values.squeeze(0),\n",
    "                'input_ids': text_inputs.input_ids,\n",
    "                'attention_mask': text_inputs.attention_mask,\n",
    "                'id': row[\"ID\"]\n",
    "            }\n",
    "\n",
    "            if self.is_train:\n",
    "                label_to_index = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "                result['labels'] = torch.tensor(label_to_index[row[\"answer\"]], dtype=torch.long)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"배치 데이터를 적절히 결합\"\"\"\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    # 배치 내 모든 텐서의 크기를 맞춤\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "\n",
    "    # 텍스트 입력들의 최대 길이 찾기\n",
    "    max_length = 0\n",
    "    for item in batch:\n",
    "        max_length = max(max_length, item['input_ids'].shape[1])\n",
    "\n",
    "    # 패딩을 적용하여 모든 텍스트 입력의 길이를 맞춤\n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "\n",
    "    for item in batch:\n",
    "        input_ids = item['input_ids']  # shape: (4, seq_len)\n",
    "        attention_mask = item['attention_mask']  # shape: (4, seq_len)\n",
    "\n",
    "        current_length = input_ids.shape[1]\n",
    "        if current_length < max_length:\n",
    "            # 패딩 적용\n",
    "            pad_size = max_length - current_length\n",
    "            input_ids = torch.nn.functional.pad(input_ids, (0, pad_size), value=0)\n",
    "            attention_mask = torch.nn.functional.pad(attention_mask, (0, pad_size), value=0)\n",
    "\n",
    "        padded_input_ids.append(input_ids)\n",
    "        padded_attention_mask.append(attention_mask)\n",
    "\n",
    "    # 배치 차원으로 결합\n",
    "    all_input_ids = torch.cat(padded_input_ids, dim=0)  # shape: (batch_size * 4, max_length)\n",
    "    all_attention_mask = torch.cat(padded_attention_mask, dim=0)  # shape: (batch_size * 4, max_length)\n",
    "\n",
    "    ids = [item['id'] for item in batch]\n",
    "\n",
    "    result = {\n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': all_input_ids,\n",
    "        'attention_mask': all_attention_mask,\n",
    "        'ids': ids\n",
    "    }\n",
    "\n",
    "    if 'labels' in batch[0]:\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        result['labels'] = labels\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecd2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPVQAModel(nn.Module):\n",
    "    \"\"\"CLIP 기반 VQA 모델\"\"\"\n",
    "    def __init__(self, clip_model_name, num_choices=4, lora_config=None):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(\n",
    "            clip_model_name,\n",
    "            use_safetensors=False,\n",
    "            #load_in_4bit=True,  # 또는 load_in_8bit=True\n",
    "            #device_map=\"auto\",\n",
    "            )\n",
    "        self.num_choices = num_choices\n",
    "\n",
    "        #self.clip_model = prepare_model_for_kbit_training(self.clip_model)\n",
    "\n",
    "            # LoRA 적용\n",
    "        self.clip_model = get_peft_model(self.clip_model, lora_config)\n",
    "\n",
    "        print(\"LoRA 모델 정보:\")\n",
    "        self.clip_model.print_trainable_parameters()\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        # 이미지 특성 추출\n",
    "        image_features = self.clip_model.get_image_features(pixel_values)\n",
    "\n",
    "        # 텍스트 특성 추출\n",
    "        text_features = self.clip_model.get_text_features(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # 특성 정규화\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        # 텍스트 특성을 (batch_size, num_choices, embedding_dim)로 재구성\n",
    "        text_features = text_features.view(batch_size, self.num_choices, -1)\n",
    "\n",
    "        # 유사도 계산\n",
    "        # image_features: (batch_size, embedding_dim)\n",
    "        # text_features: (batch_size, num_choices, embedding_dim)\n",
    "        logits = torch.bmm(\n",
    "            image_features.unsqueeze(1),\n",
    "            text_features.transpose(1, 2)\n",
    "        ).squeeze(1)\n",
    "\n",
    "        # 온도 스케일링 적용\n",
    "        logit_scale = self.clip_model.logit_scale.exp()\n",
    "        logits = logit_scale * logits\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_config(config):\n",
    "    \"\"\"LoRA 설정 생성\"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        target_modules=config.lora_target_modules,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    )\n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(config.test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(config.processor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VQADataset(test_df, processor, config.image_base_path, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2\n",
    "    )\n",
    "lora_config = create_lora_config(config)\n",
    "    # 모델 생성\n",
    "model = CLIPVQAModel(config.model_name, config.num_choices,lora_config).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "if config.save_best_model and os.path.exists(config.model_save_path):\n",
    "        model.load_state_dict(torch.load(config.model_save_path))\n",
    "        \n",
    "\n",
    "    # 테스트 추론\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Starting Inference...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            pixel_values = batch['pixel_values'].to(config.device)\n",
    "            input_ids = batch['input_ids'].to(config.device)\n",
    "            attention_mask = batch['attention_mask'].to(config.device)\n",
    "\n",
    "            logits = model(pixel_values, input_ids, attention_mask)\n",
    "            pred_indices = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            label_map = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "            batch_predictions = [label_map[idx] for idx in pred_indices]\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            ids.extend(batch['ids'])\n",
    "\n",
    "    # 제출 파일 생성\n",
    "submission_df = pd.DataFrame({'ID': ids, 'answer': predictions})\n",
    "submission_df.to_csv(config.output_submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission saved to {config.output_submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
